# âœ¨ Demo Diffusion

<p align="center">
  <img src="./resources/how-to-use-record.gif" alt="SketchMagic Studio Demo" width="500"/>
</p>

A modular, production-ready **Gradio application** for AI-powered **sketch-to-image generation** and **artistic image transformation**, built on **Stable Diffusion** and **ControlNet**.

## ğŸš€ Features

- **Sketch to Image**: Transform hand-drawn sketches into high-quality images using ControlNet
- **Magic Transformations**: Apply artistic styles and modifications to generated images using InstructPix2Pix
- **Modular Architecture**: Maintainable code structure
- **GPU Optimization**: Automatic VRAM detection and memory management
- **Quick Prompts**: Pre-built example prompts and styles for rapid experimentation

## ğŸ“ Project Structure

```
demo-diffusion/
â”œâ”€â”€ app.py                    # Main application entry point
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app_config.py        # Configuration and hyperparameters
â”‚   â””â”€â”€ constants.py         # Constants and example data
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_manager.py     # Model loading and management
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ image_processing.py  # Image preprocessing functions
â”‚   â””â”€â”€ generation.py       # Core generation logic
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ styles.py           # CSS styles
â”‚   â”œâ”€â”€ components.py       # Reusable UI components
â”‚   â”œâ”€â”€ sketch_tab.py       # Sketch to image tab
â”‚   â””â”€â”€ transform_tab.py    # Magic transformations tab
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md              # This file
```

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (recommended, 6GB+ VRAM)
- Git

### Setup Steps

1. **Clone the repository:**
```bash
git clone https://github.com/Mezosky/demo_diffusion.git
cd demo_diffusion
```

2. **Install dependencies:**
```bash
conda env create -f environment.yml
```

3. **Run the app!!**
```bash
conda activate aiartistic
python app.py
```


### Environment Variables

Set these environment variables to customize behavior:

```bash
export CUDA_VISIBLE_DEVICES=0  # Select specific GPU
export TORCH_HOME=/path/to/models  # Custom model cache directory
```

## ğŸ¯ Usage Guide

### Sketch to Image

1. Draw a sketch on the canvas using the drawing tools
2. Enter a detailed text prompt describing your vision
3. Adjust generation parameters:
   - **Guidance Scale**: How closely to follow the prompt (7.5 recommended)
   - **Sketch Influence**: How much the sketch controls output (1.0 recommended)
   - **Quality Steps**: More steps = higher quality but slower (20 recommended)
   - **Seed**: Set for reproducible results (-1 for random)
4. Click "ğŸš€ Generate Image"

### Magic Transformations

1. First generate an image in the Sketch to Image tab
2. Enter transformation instructions (e.g., "Transform into Van Gogh style")
3. Adjust transformation parameters:
   - **Text Guidance**: How closely to follow instructions (7.5 recommended)
   - **Image Preservation**: How much to preserve original structure (1.5 recommended)
   - **Quality Steps**: Processing steps (20 recommended)
4. Click "ğŸŒŸ Apply Transform"

## âš™ï¸ Configuration

### Model Configuration

Edit `config/app_config.py` to change model settings:

```python
class AppConfig:
    # Model IDs
    CONTROLNET_MODEL_ID = "lllyasviel/sd-controlnet-scribble"
    STABLE_DIFFUSION_MODEL_ID = "runwayml/stable-diffusion-v1-5"
    INSTRUCTPIX2PIX_MODEL_ID = "timbrooks/instruct-pix2pix"
    
    # Default parameters
    SKETCH_HYPERPARAMS = {
        "guidance_scale": 7.5,
        "num_inference_steps": 20,
        "controlnet_conditioning_scale": 1.0,
        "seed": -1,
    }
```

---

**Happy Sketching! ğŸ¨âœ¨**